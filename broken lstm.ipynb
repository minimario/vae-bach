{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vae music21?",
      "provenance": [],
      "authorship_tag": "ABX9TyP5XzfINPmXhrDNqDsHn8Nh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minimario/vae-bach/blob/main/broken%20lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "ziGZ655E5weB",
        "outputId": "537be127-8713-4d08-b87c-2bba9c24ed8e"
      },
      "source": [
        "%pip install --upgrade music21"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting music21\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/0e/b9bf3530203f6e6ed1f04d4352ac421aef2429ab77c416ff583dd6d58597/music21-6.3.0.tar.gz (19.2MB)\n",
            "\u001b[K     |████████████████████████████████| 19.2MB 1.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: chardet in /usr/local/lib/python3.6/dist-packages (from music21) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.6/dist-packages (from music21) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: more-itertools in /usr/local/lib/python3.6/dist-packages (from music21) (8.6.0)\n",
            "Collecting webcolors\n",
            "  Downloading https://files.pythonhosted.org/packages/12/05/3350559de9714b202e443a9e6312937341bd5f79f4e4f625744295e7dd17/webcolors-1.11.1-py3-none-any.whl\n",
            "Building wheels for collected packages: music21\n",
            "  Building wheel for music21 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for music21: filename=music21-6.3.0-cp36-none-any.whl size=21888021 sha256=f34dad481f42726171089a178b2702c0eab6962df92764db71cb0dd2f8367e6d\n",
            "  Stored in directory: /root/.cache/pip/wheels/02/e8/2c/eed32afec2b6c6f945a17280c4e4df1cf2e8cd15ebe1025680\n",
            "Successfully built music21\n",
            "Installing collected packages: webcolors, music21\n",
            "  Found existing installation: music21 5.5.0\n",
            "    Uninstalling music21-5.5.0:\n",
            "      Successfully uninstalled music21-5.5.0\n",
            "Successfully installed music21-6.3.0 webcolors-1.11.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "music21"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bf9XWqc6GDp",
        "outputId": "f1f79ec9-2f6c-49c5-955f-da1f889d5dd9"
      },
      "source": [
        "print(music21.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55_THJcNdYwn"
      },
      "source": [
        "import music21\n",
        "import music21 as m21\n",
        "from music21 import *\n",
        "import keras\n",
        "import numpy as np\n",
        "paths = corpus.getComposer('bach')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qh9zviWYdarC"
      },
      "source": [
        "bach = []\n",
        "for i in paths:\n",
        "    s = corpus.parse(i)\n",
        "    bach.append(s)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JudlP6-UdcMl"
      },
      "source": [
        "def transpose(song):\n",
        "    parts = song.getElementsByClass(music21.stream.Part)\n",
        "    measures_part0 = parts[0].getElementsByClass(music21.stream.Measure)\n",
        "    \n",
        "    key = measures_part0[0].getElementsByClass(music21.key.Key)\n",
        "    \n",
        "    if len(key) != 0:\n",
        "        key = key[0]\n",
        "    else:\n",
        "        key = song.analyze(\"key\")\n",
        "\n",
        "    # get interval for transposition. E.g., Bmaj -> Cmaj\n",
        "    if key.mode == \"major\":\n",
        "        interval = music21.interval.Interval(key.tonic, music21.pitch.Pitch(\"C\"))\n",
        "    elif key.mode == \"minor\":\n",
        "        interval = music21.interval.Interval(key.tonic, music21.pitch.Pitch(\"A\"))\n",
        "\n",
        "    # transpose song by calculated interval\n",
        "    tranposed_song = song.transpose(interval)\n",
        "    return tranposed_song"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRBXFZgCddkc"
      },
      "source": [
        "def encode_song(song, time_step=0.25):\n",
        "    \"\"\"Converts a score into a time-series-like music representation. Each item in the encoded list represents 'min_duration'\n",
        "    quarter lengths. The symbols used at each step are: integers for MIDI notes, 'r' for representing a rest, and '_'\n",
        "    for representing notes/rests that are carried over into a new time step. Here's a sample encoding:\n",
        "\n",
        "        [\"r\", \"_\", \"60\", \"_\", \"_\", \"_\", \"72\" \"_\"]\n",
        "\n",
        "    :param song (m21 stream): Piece to encode\n",
        "    :param time_step (float): Duration of each time step in quarter length\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    encoded_song = []\n",
        "\n",
        "    for event in song.flat.notesAndRests:\n",
        "\n",
        "        # handle notes\n",
        "        if isinstance(event, m21.note.Note):\n",
        "            symbol = event.pitch.midi # 60\n",
        "        # handle rests\n",
        "        elif isinstance(event, m21.note.Rest):\n",
        "            symbol = \"r\"\n",
        "\n",
        "        # convert the note/rest into time series notation\n",
        "        steps = int(event.duration.quarterLength / time_step)\n",
        "        for step in range(steps):\n",
        "\n",
        "            # if it's the first time we see a note/rest, let's encode it. Otherwise, it means we're carrying the same\n",
        "            # symbol in a new time step\n",
        "            if step == 0:\n",
        "                encoded_song.append(symbol)\n",
        "            else:\n",
        "                encoded_song.append(\"_\")\n",
        "\n",
        "    # cast encoded song to str\n",
        "    encoded_song = list(map(str, encoded_song))\n",
        "\n",
        "    return encoded_song\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P67tlf3hdgLf"
      },
      "source": [
        "bach_transposed = []\n",
        "for s in bach:\n",
        "    bach_transposed.append(transpose(s))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQKyv88AdhL-"
      },
      "source": [
        "def transpose_octave(encoding, direction):\n",
        "    \"\"\"\n",
        "    takes in a string encoding and transposes it up/down an octave\n",
        "    direction: one of 'up', 'down'\n",
        "    \"\"\"\n",
        "    transposed_encoding = []\n",
        "    for i in encoding:\n",
        "        if i in '_r':\n",
        "            transposed_encoding.append(i)\n",
        "        else:\n",
        "            if direction == 'up':\n",
        "                transposed_encoding.append(str(int(i)+12))\n",
        "            else:\n",
        "                transposed_encoding.append(str(int(i)-12))\n",
        "    return transposed_encoding"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YpKujyfdiAW"
      },
      "source": [
        "encodings = []\n",
        "\n",
        "for b in bach_transposed:\n",
        "    for part in b.parts:\n",
        "      if part.partName in ['Soprano', 'Tenor', 'Alto', 'Bass']:\n",
        "        part_encoded = encode_song(part)\n",
        "        if part.partName == 'Soprano':\n",
        "            encodings.append(transpose_octave(part_encoded, 'down'))\n",
        "        elif part.partName == 'Bass':\n",
        "            encodings.append(transpose_octave(part_encoded, 'up'))\n",
        "        else:\n",
        "            encodings.append(part_encoded)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pd1-aWSedixZ"
      },
      "source": [
        "import itertools\n",
        "mappings = {}\n",
        "inv_mappings = {}\n",
        "distinct_symbols = list(set(itertools.chain.from_iterable(encodings)))\n",
        "for i, s in enumerate(distinct_symbols):\n",
        "    mappings[s] = i\n",
        "    inv_mappings[i] = s"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0Twku8Xdjub"
      },
      "source": [
        "def compress(songs):\n",
        "  return [mappings[symbol] for symbol in songs]\n",
        "\n",
        "def decompress(ints):\n",
        "  return [inv_mappings[num] for num in ints]\n",
        "  "
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOH5IDgGdkjN"
      },
      "source": [
        "mapped_encodings = list(map(compress, encodings))"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9O1UTpEFNjuR",
        "outputId": "efeae390-1007-442a-ff66-1a8e972e1704"
      },
      "source": [
        "print(len_64_segments[0])\n",
        "print(len_64_segments[1])\n",
        "len(mapped_encodings[0])"
      ],
      "execution_count": 235,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[21, 39, 39, 39, 24, 39, 39, 39, 1, 39, 39, 39, 21, 39, 39, 39, 24, 39, 39, 39, 16, 39, 39, 39, 16, 39, 39, 39, 24, 39, 39, 39, 24, 39, 39, 39, 16, 39, 39, 39, 34, 39, 39, 39, 9, 39, 39, 39, 34, 39, 39, 39, 16, 39, 39, 39, 16, 39, 39, 39, 24, 39, 39, 39]\n",
            "[39, 39, 39, 24, 39, 39, 39, 1, 39, 39, 39, 21, 39, 39, 39, 24, 39, 39, 39, 16, 39, 39, 39, 16, 39, 39, 39, 24, 39, 39, 39, 24, 39, 39, 39, 16, 39, 39, 39, 34, 39, 39, 39, 9, 39, 39, 39, 34, 39, 39, 39, 16, 39, 39, 39, 16, 39, 39, 39, 24, 39, 39, 39, 1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "320"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 235
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWxYGs7idlXd"
      },
      "source": [
        "len_64_segments = []\n",
        "targets = []\n",
        "for encoding in mapped_encodings:\n",
        "    for j in range(0, len(encoding)):\n",
        "        if j+65 <= len(encoding):\n",
        "            len_64_segments.append(encoding[j:j+64])\n",
        "            targets.append(encoding[j+64])"
      ],
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_54GjiHdnE0",
        "outputId": "7f17d06f-f75e-430d-dd01-b6acb00ef9ef"
      },
      "source": [
        "print(len(mappings))\n",
        "inputs = np.array(len_64_segments)\n",
        "targets = np.array(targets)\n",
        "inputs_cat = keras.utils.to_categorical(inputs)\n",
        "targets_cat = keras.utils.to_categorical(targets)\n",
        "print(inputs_cat.shape)\n",
        "print(targets_cat.shape)\n"
      ],
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "45\n",
            "(264103, 64, 45)\n",
            "(264103, 45)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sN0LtI0fLdg",
        "outputId": "2e06697f-2646-4dc2-d62f-49bd7400d2f4"
      },
      "source": [
        "inputs_cat = inputs_cat.reshape((inputs_cat.shape[0], -1, 1))\n",
        "print(inputs_cat.shape)"
      ],
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(264103, 2880, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQ9ZdDPOJjFX",
        "outputId": "e12f33dd-26e5-4965-d58b-fb5a9639260a"
      },
      "source": [
        "input = keras.layers.Input(shape=(None, 45))\n",
        "x = keras.layers.LSTM(256)(input)\n",
        "x = keras.layers.Dropout(0.2)(x)\n",
        "output = keras.layers.Dense(45, activation=\"softmax\")(x)\n",
        "model = keras.Model(input, output)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "print(inputs_cat.shape)\n",
        "print(targets_cat.shape)\n",
        "model.fit(inputs_cat, targets, epochs=10, batch_size=128)"
      ],
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_22 (InputLayer)        [(None, None, 45)]        0         \n",
            "_________________________________________________________________\n",
            "lstm_8 (LSTM)                (None, 256)               309248    \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 45)                11565     \n",
            "=================================================================\n",
            "Total params: 320,813\n",
            "Trainable params: 320,813\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "(264103, 64, 45)\n",
            "(264103, 45)\n",
            "Epoch 1/10\n",
            "2064/2064 [==============================] - 53s 25ms/step - loss: 1.1607 - accuracy: 0.7370\n",
            "Epoch 2/10\n",
            "2064/2064 [==============================] - 51s 25ms/step - loss: 0.7806 - accuracy: 0.7633\n",
            "Epoch 3/10\n",
            "2064/2064 [==============================] - 51s 25ms/step - loss: 0.7374 - accuracy: 0.7754\n",
            "Epoch 4/10\n",
            "2064/2064 [==============================] - 51s 25ms/step - loss: 0.7132 - accuracy: 0.7833\n",
            "Epoch 5/10\n",
            "2064/2064 [==============================] - 51s 25ms/step - loss: 0.6847 - accuracy: 0.7903\n",
            "Epoch 6/10\n",
            "2064/2064 [==============================] - 51s 25ms/step - loss: 0.6724 - accuracy: 0.7936\n",
            "Epoch 7/10\n",
            "2064/2064 [==============================] - 51s 25ms/step - loss: 0.6521 - accuracy: 0.8002\n",
            "Epoch 8/10\n",
            "2064/2064 [==============================] - 51s 25ms/step - loss: 0.6398 - accuracy: 0.8030\n",
            "Epoch 9/10\n",
            "2064/2064 [==============================] - 51s 25ms/step - loss: 0.6313 - accuracy: 0.8050\n",
            "Epoch 10/10\n",
            "2064/2064 [==============================] - 51s 25ms/step - loss: 0.6180 - accuracy: 0.8088\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7d4620ad30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 244
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8w3fM6sZLzbW"
      },
      "source": [
        "def sample(probabilites, temperature):\n",
        "    \"\"\"Samples an index from a probability array reapplying softmax using temperature\n",
        "\n",
        "    :param predictions (nd.array): Array containing probabilities for each of the possible outputs.\n",
        "    :param temperature (float): Float in interval [0, 1]. Numbers closer to 0 make the model more deterministic.\n",
        "        A number closer to 1 makes the generation more unpredictable.\n",
        "\n",
        "    :return index (int): Selected output symbol\n",
        "    \"\"\"\n",
        "    predictions = np.log(probabilites) / temperature\n",
        "    probabilites = np.exp(predictions) / np.sum(np.exp(predictions))\n",
        "\n",
        "    choices = range(len(probabilites)) # [0, 1, 2, 3]\n",
        "    index = np.random.choice(choices, p=probabilites)\n",
        "\n",
        "    return index\n"
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wa8vHdpmKvvp",
        "outputId": "4df766fb-c163-4651-d59c-befe935518a3"
      },
      "source": [
        "seed = ['64', '_', '_', '_']\n",
        "melody = seed\n",
        "seed = [mappings[symbol] for symbol in seed]\n",
        "for i in range(100):\n",
        "  print(seed)\n",
        "  onehot_seed = keras.utils.to_categorical(seed, num_classes=len(mappings))\n",
        "  onehot_seed = onehot_seed[np.newaxis, ...]\n",
        "  probabilities = model.predict(onehot_seed)[0]\n",
        "  next = sample(probabilities, .3)\n",
        "  seed.append(next)\n",
        "  melody.append(inv_mappings[next])\n",
        "\n",
        "print(seed)\n",
        "s = save_melody(melody)"
      ],
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[31, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n",
            "[31, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xtpfmdJMm2I",
        "outputId": "fe521575-e379-4e58-cd8c-d433f3e75601"
      },
      "source": [
        "for i in s:\n",
        "  print(i)"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<music21.note.Note E>\n",
            "<music21.note.Note D>\n",
            "<music21.note.Note E>\n",
            "<music21.note.Note C#>\n",
            "<music21.note.Note D>\n",
            "<music21.note.Note C#>\n",
            "<music21.note.Note D>\n",
            "<music21.note.Note C#>\n",
            "<music21.note.Note D>\n",
            "<music21.note.Note C#>\n",
            "<music21.note.Note D>\n",
            "<music21.note.Note C#>\n",
            "<music21.note.Note D>\n",
            "<music21.note.Note D>\n",
            "<music21.note.Note E>\n",
            "<music21.note.Note F>\n",
            "<music21.note.Note E>\n",
            "<music21.note.Note F>\n",
            "<music21.note.Note E>\n",
            "<music21.note.Note F>\n",
            "<music21.note.Note E>\n",
            "<music21.note.Note F>\n",
            "<music21.note.Note E>\n",
            "<music21.note.Note F>\n",
            "<music21.note.Note E>\n",
            "<music21.note.Note F>\n",
            "<music21.note.Note E>\n",
            "<music21.note.Note F>\n",
            "<music21.note.Note E>\n",
            "<music21.note.Note F>\n",
            "<music21.note.Note E>\n",
            "<music21.note.Note D>\n",
            "<music21.note.Note C#>\n",
            "<music21.note.Note D>\n",
            "<music21.note.Note C>\n",
            "<music21.note.Note D>\n",
            "<music21.note.Note C>\n",
            "<music21.note.Note B>\n",
            "<music21.note.Note C>\n",
            "<music21.note.Note B>\n",
            "<music21.note.Note C>\n",
            "<music21.note.Note B>\n",
            "<music21.note.Note C>\n",
            "<music21.note.Note B>\n",
            "<music21.note.Note C>\n",
            "<music21.note.Note B>\n",
            "<music21.note.Note C>\n",
            "<music21.note.Note B>\n",
            "<music21.note.Note C>\n",
            "<music21.note.Note B>\n",
            "<music21.note.Note C>\n",
            "<music21.note.Note B>\n",
            "<music21.note.Note C>\n",
            "<music21.note.Note B>\n",
            "<music21.note.Note C>\n",
            "<music21.note.Note B>\n",
            "<music21.note.Note C>\n",
            "<music21.note.Note B>\n",
            "<music21.note.Note C>\n",
            "<music21.note.Note B>\n",
            "<music21.note.Note C>\n",
            "<music21.note.Note B>\n",
            "<music21.note.Note C>\n",
            "<music21.note.Note B>\n",
            "<music21.note.Note C>\n",
            "<music21.note.Note B>\n",
            "<music21.note.Note C>\n",
            "<music21.note.Note B>\n",
            "<music21.note.Note C>\n",
            "<music21.note.Note B>\n",
            "<music21.note.Note F>\n",
            "<music21.note.Note E>\n",
            "<music21.note.Note E>\n",
            "<music21.note.Note F#>\n",
            "<music21.note.Note D>\n",
            "<music21.note.Note G>\n",
            "<music21.note.Note G>\n",
            "<music21.note.Note F#>\n",
            "<music21.note.Note E>\n",
            "<music21.note.Note F#>\n",
            "<music21.note.Note D>\n",
            "<music21.note.Note D>\n",
            "<music21.note.Note E>\n",
            "<music21.note.Note D>\n",
            "<music21.note.Note C>\n",
            "<music21.note.Note B>\n",
            "<music21.note.Note B>\n",
            "<music21.note.Note C>\n",
            "<music21.note.Note B>\n",
            "<music21.note.Note C>\n",
            "<music21.note.Note B>\n",
            "<music21.note.Note C>\n",
            "<music21.note.Note B>\n",
            "<music21.note.Note C>\n",
            "<music21.note.Note B>\n",
            "<music21.note.Note C>\n",
            "<music21.note.Note B>\n",
            "<music21.note.Note C>\n",
            "<music21.note.Note B>\n",
            "<music21.note.Note C>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiYlQfpIe9Em"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class Sampling(layers.Layer):\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgfmGcPafIuR",
        "outputId": "2dc5c825-f934-490f-95c0-82dce66e740b"
      },
      "source": [
        "input_shape = inputs_cat.shape[1]\n",
        "\n",
        "latent_dim = 256\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(input_shape,1))\n",
        "x = layers.Conv1D(32, 8, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Conv1D(64, 8, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(16, activation=\"relu\")(x)\n",
        "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
        "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
        "z = Sampling()([z_mean, z_log_var])\n",
        "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "encoder.summary()"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_11 (InputLayer)           [(None, 2880, 1)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_10 (Conv1D)              (None, 1440, 32)     288         input_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 1440, 32)     128         conv1d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_11 (Conv1D)              (None, 720, 64)      16448       batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 720, 64)      256         conv1d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_4 (Flatten)             (None, 46080)        0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 16)           737296      flatten_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "z_mean (Dense)                  (None, 256)          4352        dense_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "z_log_var (Dense)               (None, 256)          4352        dense_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "sampling_4 (Sampling)           (None, 256)          0           z_mean[0][0]                     \n",
            "                                                                 z_log_var[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 763,120\n",
            "Trainable params: 762,928\n",
            "Non-trainable params: 192\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7XOu5usfhT7",
        "outputId": "4f48b325-5e48-4ac8-da29-48fa69c43f52"
      },
      "source": [
        "latent_inputs = keras.Input(shape=(latent_dim,))\n",
        "x = layers.Dense(720 * 64, activation=\"relu\")(latent_inputs)\n",
        "x = layers.Reshape((720, 64))(x)\n",
        "x = layers.Conv1DTranspose(64, 8, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Conv1DTranspose(32, 8, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "decoder_outputs = layers.Conv1DTranspose(1, 8, activation=\"sigmoid\", padding=\"same\")(x)\n",
        "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
        "decoder.summary()"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_12 (InputLayer)        [(None, 256)]             0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 46080)             11842560  \n",
            "_________________________________________________________________\n",
            "reshape_4 (Reshape)          (None, 720, 64)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_transpose_12 (Conv1DT (None, 1440, 64)          32832     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 1440, 64)          256       \n",
            "_________________________________________________________________\n",
            "conv1d_transpose_13 (Conv1DT (None, 2880, 32)          16416     \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 2880, 32)          128       \n",
            "_________________________________________________________________\n",
            "conv1d_transpose_14 (Conv1DT (None, 2880, 1)           257       \n",
            "=================================================================\n",
            "Total params: 11,892,449\n",
            "Trainable params: 11,892,257\n",
            "Non-trainable params: 192\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEWrwZc8fivm"
      },
      "source": [
        "class VAE(keras.Model):\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super(VAE, self).__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def train_step(self, data):\n",
        "        if isinstance(data, tuple):\n",
        "            data = data[0]\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var, z = encoder(data)\n",
        "            reconstruction = decoder(z)\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                keras.losses.binary_crossentropy(data, reconstruction)\n",
        "            )\n",
        "            kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
        "            kl_loss = tf.reduce_mean(kl_loss)\n",
        "            kl_loss *= -0.5\n",
        "            recon_weight = 0.8\n",
        "            total_loss = recon_weight * reconstruction_loss + (1 - recon_weight) * kl_loss\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        return {\n",
        "            \"loss\": total_loss,\n",
        "            \"reconstruction_loss\": reconstruction_loss,\n",
        "            \"kl_loss\": kl_loss,\n",
        "        }"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeizfaxNHSEq",
        "outputId": "f0eeb07b-77cb-43c5-c846-b7320495e6a6"
      },
      "source": [
        "for i in range(100):\n",
        "  out = encoder.predict(vae_inputs[i].reshape(1, 2880, 1))\n",
        "  print(np.mean(out), np.var(out))"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-0.050523657 0.30552134\n",
            "0.009959507 0.350953\n",
            "0.02210316 0.35578656\n",
            "-0.028623095 0.32600254\n",
            "-0.010680961 0.3144131\n",
            "-0.0172643 0.31119087\n",
            "-0.013688301 0.30116317\n",
            "-0.027119061 0.37813613\n",
            "0.0035797544 0.34352347\n",
            "0.012591652 0.31378987\n",
            "0.008993582 0.3509201\n",
            "-0.030732453 0.34387088\n",
            "0.007668776 0.3526442\n",
            "-0.041668624 0.32000604\n",
            "0.009354499 0.361106\n",
            "0.012860139 0.37052977\n",
            "-0.050093036 0.35526612\n",
            "-0.00803714 0.31331807\n",
            "0.01524581 0.31517193\n",
            "0.00013221164 0.36427176\n",
            "-0.02844514 0.3232139\n",
            "-0.0027470598 0.3222799\n",
            "-6.951097e-05 0.28267705\n",
            "-0.012141583 0.32975066\n",
            "0.0001789052 0.32071587\n",
            "0.012982859 0.3581239\n",
            "-0.0029055264 0.3423823\n",
            "-0.019416427 0.34267583\n",
            "-0.037826534 0.34019366\n",
            "-0.0011096309 0.3221682\n",
            "-0.03528048 0.36129895\n",
            "-0.0014566928 0.33851445\n",
            "-0.02397195 0.3156367\n",
            "0.0075398125 0.33713815\n",
            "0.012631208 0.307344\n",
            "-0.0060704923 0.25659046\n",
            "-0.013427139 0.2948809\n",
            "0.011204724 0.31350937\n",
            "-0.033050384 0.36321315\n",
            "0.020322097 0.30954\n",
            "0.01843811 0.3379905\n",
            "0.013128032 0.3126481\n",
            "0.020200187 0.31608918\n",
            "0.0070111738 0.29234508\n",
            "-0.03952458 0.38059187\n",
            "-0.010411573 0.33470914\n",
            "-0.011721172 0.32592097\n",
            "-0.036531735 0.33634457\n",
            "0.0108188465 0.3077131\n",
            "0.022210158 0.32802093\n",
            "0.010005382 0.37420332\n",
            "-0.016013514 0.32240492\n",
            "0.018521292 0.34316912\n",
            "-0.01982095 0.3420787\n",
            "-0.024797091 0.36440554\n",
            "0.017651744 0.309753\n",
            "-0.024125436 0.31994352\n",
            "0.022661202 0.33315042\n",
            "-0.009134662 0.36338055\n",
            "0.004381998 0.3483083\n",
            "0.0064875106 0.32731265\n",
            "0.023300072 0.3213193\n",
            "0.024617812 0.28352904\n",
            "-0.0082254335 0.391024\n",
            "0.016455904 0.30221316\n",
            "-0.045948386 0.40085402\n",
            "0.015736278 0.25923577\n",
            "0.024712518 0.33918086\n",
            "-0.007877671 0.333526\n",
            "0.0012501286 0.3658795\n",
            "-0.022958368 0.34245142\n",
            "-0.0085253 0.3340342\n",
            "-0.0025606886 0.33779785\n",
            "-0.005920721 0.29353896\n",
            "-0.013173915 0.2842082\n",
            "0.0007874824 0.3025652\n",
            "-0.015468545 0.32048818\n",
            "-0.011361242 0.34168386\n",
            "0.005704938 0.36481592\n",
            "0.0028203211 0.34309182\n",
            "-0.015390328 0.35302234\n",
            "0.009335985 0.34899774\n",
            "-0.045493606 0.27278012\n",
            "0.0042458833 0.36915493\n",
            "-0.0109437285 0.32520917\n",
            "0.008117202 0.31852534\n",
            "-0.023122465 0.27552494\n",
            "-0.008868791 0.25152436\n",
            "-0.013238848 0.34874877\n",
            "-0.0044852006 0.31748635\n",
            "0.01478772 0.31638792\n",
            "-0.0070508923 0.27997348\n",
            "0.0010102612 0.32222655\n",
            "0.002381134 0.31635362\n",
            "0.014513162 0.30478776\n",
            "-0.02354869 0.3236865\n",
            "-0.015839418 0.32899946\n",
            "-0.019724479 0.3160421\n",
            "0.0022059705 0.30366105\n",
            "0.01375635 0.34952995\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 795
        },
        "id": "M1dgMDwTgDrM",
        "outputId": "c26bf0f1-5687-4a65-b8e7-de267b20eb5a"
      },
      "source": [
        "vae_inputs = inputs_cat[:, :, np.newaxis]\n",
        "vae = VAE(encoder, decoder)\n",
        "vae.compile(optimizer=keras.optimizers.Adam())\n",
        "vae.fit(vae_inputs, epochs=30, batch_size=128)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "448/448 [==============================] - 46s 99ms/step - loss: 0.0487 - reconstruction_loss: 0.0578 - kl_loss: 0.0126\n",
            "Epoch 2/30\n",
            "448/448 [==============================] - 44s 99ms/step - loss: 0.0283 - reconstruction_loss: 0.0338 - kl_loss: 0.0064\n",
            "Epoch 3/30\n",
            "448/448 [==============================] - 44s 99ms/step - loss: 0.0266 - reconstruction_loss: 0.0326 - kl_loss: 0.0029\n",
            "Epoch 4/30\n",
            "448/448 [==============================] - 44s 99ms/step - loss: 0.0256 - reconstruction_loss: 0.0312 - kl_loss: 0.0033\n",
            "Epoch 5/30\n",
            "448/448 [==============================] - 44s 99ms/step - loss: 0.0253 - reconstruction_loss: 0.0307 - kl_loss: 0.0036\n",
            "Epoch 6/30\n",
            "448/448 [==============================] - 44s 98ms/step - loss: 0.0252 - reconstruction_loss: 0.0305 - kl_loss: 0.0038\n",
            "Epoch 7/30\n",
            "448/448 [==============================] - 44s 98ms/step - loss: 0.0251 - reconstruction_loss: 0.0304 - kl_loss: 0.0039\n",
            "Epoch 8/30\n",
            "448/448 [==============================] - 44s 98ms/step - loss: 0.0250 - reconstruction_loss: 0.0303 - kl_loss: 0.0040\n",
            "Epoch 9/30\n",
            "448/448 [==============================] - 44s 98ms/step - loss: 0.0250 - reconstruction_loss: 0.0302 - kl_loss: 0.0040\n",
            "Epoch 10/30\n",
            "448/448 [==============================] - 44s 98ms/step - loss: 0.0250 - reconstruction_loss: 0.0302 - kl_loss: 0.0041\n",
            "Epoch 11/30\n",
            "448/448 [==============================] - 44s 98ms/step - loss: 0.0249 - reconstruction_loss: 0.0301 - kl_loss: 0.0041\n",
            "Epoch 12/30\n",
            " 91/448 [=====>........................] - ETA: 35s - loss: 0.0250 - reconstruction_loss: 0.0302 - kl_loss: 0.0042"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-101-cff55676b930>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvae_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-qyIvUF9IXs"
      },
      "source": [
        "def save_melody(melody, step_duration=0.25, format=\"midi\", file_name=\"mel.mid\"):\n",
        "    \"\"\"Converts a melody into a MIDI file\n",
        "\n",
        "    :param melody (list of str):\n",
        "    :param min_duration (float): Duration of each time step in quarter length\n",
        "    :param file_name (str): Name of midi file\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    # create a music21 stream\n",
        "    stream = m21.stream.Stream()\n",
        "\n",
        "    start_symbol = None\n",
        "    step_counter = 1\n",
        "\n",
        "    # parse all the symbols in the melody and create note/rest objects\n",
        "    for i, symbol in enumerate(melody):\n",
        "        # handle case in which we have a note/rest\n",
        "        if symbol != \"_\" or i + 1 == len(melody):\n",
        "            # ensure we're dealing with note/rest beyond the first one\n",
        "            if start_symbol is not None:\n",
        "                quarter_length_duration = step_duration * step_counter # 0.25 * 4 = 1\n",
        "                # handle rest\n",
        "                if start_symbol == \"r\":\n",
        "                    m21_event = m21.note.Rest(quarterLength=quarter_length_duration)\n",
        "                # handle note\n",
        "                else:\n",
        "                    m21_event = m21.note.Note(int(start_symbol), quarterLength=quarter_length_duration)\n",
        "                stream.append(m21_event)\n",
        "                # reset the step counter\n",
        "                step_counter = 1\n",
        "            start_symbol = symbol\n",
        "        # handle case in which we have a prolongation sign \"_\"\n",
        "        else:\n",
        "            step_counter += 1\n",
        "    # write the m21 stream to a midi file\n",
        "    stream.write(format, file_name)\n",
        "    return stream"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nnj8Jlme-Iqk",
        "outputId": "d6750a49-2278-4017-c1ed-8d57d66a83ed"
      },
      "source": [
        "for i in range(10):\n",
        "  latent_input = np.random.normal(0, .3, (1, latent_dim))\n",
        "  decoded = decoder.predict(latent_input).reshape((64, 45))\n",
        "  melody = np.argmax(decoded, axis=1)\n",
        "  song = decompress(list(melody))\n",
        "  save_melody(song)\n",
        "  print(song)\n",
        "  s = save_melody(song)\n",
        "  for i in s:\n",
        "    print(i)"
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['64', '_', '_', '_', '64', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_']\n",
            "<music21.note.Note E>\n",
            "<music21.note.Note E>\n",
            "['60', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_']\n",
            "<music21.note.Note C>\n",
            "['60', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_']\n",
            "<music21.note.Note C>\n",
            "['55', '_', '_', '_', '_', '_', '_', '_', '55', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '55', '_', '_', '_', '_', '_', '_', '_', '55', '_', '_', '_', '55', '_', '_', '_', '_', '_', '_', '_', '55', '_', '_', '_', '55', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_']\n",
            "<music21.note.Note G>\n",
            "<music21.note.Note G>\n",
            "<music21.note.Note G>\n",
            "<music21.note.Note G>\n",
            "<music21.note.Note G>\n",
            "<music21.note.Note G>\n",
            "<music21.note.Note G>\n",
            "['55', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_']\n",
            "<music21.note.Note G>\n",
            "['60', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_']\n",
            "<music21.note.Note C>\n",
            "['60', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_']\n",
            "<music21.note.Note C>\n",
            "['55', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_']\n",
            "<music21.note.Note G>\n",
            "['60', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_']\n",
            "<music21.note.Note C>\n",
            "['60', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_']\n",
            "<music21.note.Note C>\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}